
const tokenizers = @[]
  @{} name: 'comment_eol', kind:'//', open: /(\/\/)/, close: /.*($)/
  @{} name: 'comment_multi', kind:'/*', open: /(\/\*)/, close: /.*?(\*\/|$)/, multiline: true
  @{} name: 'str_single', kind:"'", open: /(')/, close: /(?:\\.|[^'])*(')/
  @{} name: 'str_double', kind:'"', open: /(")/, close: /(?:\\.|[^"])*(")/
  @{} name: 'str_multi', kind:'`', open: /(`)/, close: /(?:\\.|[^`])*(`|$)/, multiline: true
  @{} name: 'empty_line', kind:'', open: /(^)/, close: /\s*($)/

const rx_tokens = new RegExp @
  tokenizers
    .map @ e => `(?:${e.open.source}${e.close.source})`
    .join('|')
  'g'

tokenizers.multiline = {}
tokenizers.kind = {}

::
  for const each of tokenizers ::
    tokenizers.kind[each.kind] = each
    tokenizers.kind[each.name] = each
    if each.multiline ::
      tokenizers.multiline[each.kind] = each


transform_jsy.func = function (func) ::
  return transform_jsy @ func.toString()
transform_jsy.file = function(filename) ::
  return new Promise @ (resolve, reject) =>
    fs.readFile @ filename, 'utf8', (err, src) =>
      err ? reject(err) : resolve @ transform_jsy @ src, filename

export default transform_jsy
async function transform_jsy(file_source, filename) ::
  const lines = file_source.split(/\r\n|\r|\n/)
    .map @ (src, idx) => Object.defineProperties @
      @: src, pos: @{} char: 0, line: 1+idx, //filename
      @: file_source: @{} value: file_source

  tokenize @ lines[0]
  lines.reduce @ (a, b) => ::
    b.pos.char = a.pos.char + a.src.length
    tokenize @ b, a.tkn_continue
    return b

  return lines


  function tokenize(entry, tkn_continue) ::
    let sz0=0, {src} = entry
    const src_orig=src, shadow = entry.shadow = []

    if null == tkn_continue ::
      entry.indent = src.match(/^\s*/)[0]
    else ::
      entry.indent = false
      src = tkn_continue @ shadow, src, 0
      if ! src ::
        entry.tkn_continue = tkn_continue
        entry.skip = true
        return entry
      sz0 = src_orig.length - src.length

    tokenize_shadows(entry, src, sz0)

    // TODO: handle JSX here

    if ! entry.skip ::
      applyOffsideOps(entry)

    return entry

  function tokenize_shadows(entry, src, sz0) ::
    const {shadow, src: src_orig} = entry
    src.replace @ rx_tokens, map_shadows
    return

    function as_tkn_continue(multi) ::
      return (shadow, src, sz0) =>
        src.replace @ multi.close, (match, ...args) => ::
          shadow.push @# 0, match.length, multi.name
          return ''

    function map_shadows(match, ...pairs) ::
      const src = pairs.pop()
      const pos = pairs.pop()
      pairs = pairs.filter @ e=> undefined !== e

      if pairs.length !== 2 ::
        throw new Error @ 'Pair mismatch'

      if ! pairs[1] ::
        const multi = tokenizers.multiline[pairs[0]]
        if multi ::
          entry.tkn_continue = as_tkn_continue(multi)

        if ! pairs[0] ::
          entry.skip = true
          entry.indent = false
          return match // no-op

      const s = sz0 + pos, e = s+match.length, orig = src_orig.slice(s,e)
      if match != orig ::
        throw new Error @ `Slice mismatch (s:${s} e:${e} m:"${match}" orig:"${orig}")`
      shadow.push @# s,e, tokenizers.kind[pairs[0]].name
      return match // no-op

  function applyOffsideOps(entry) ::
    const {shadow, src: src_orig} = entry
    console.log @ '\nOFFSIDE', entry.pos.line, JSON.stringify @ src_orig
    const offside = entry.offside = []
    var ans = src_orig.replace @ rx_offside, (match, ...args) => ::
      args.pop()
      const pos = args.pop()
      const offside_op = args.shift()
      const offside_kw = args.shift()

      const tip = shadow.find @ e => e[1] > pos
      if tip && pos >= tip[0] ::
        return match // in shadow

      if offside_op ::
        console.log @ '  JSY Op', @[] pos, offside_op, tip
        const op = at_offside[offside_op]
        offside.push @# pos, 'op', offside_op, op.post
        return op.pre

      if offside_kw ::
        offside.push @# pos, 'kw', offside_op
        return `${offside_kw}(`

    console.log @: offside
    console.log @ 'DONE OFFSIDE', JSON.stringify @ ans

const rx_offside_op = /\B(::\(\)|::\{\}|::\[\]|::|@\(\)|@\{\}|@\[\]|@:|@#|@)\B/g
const rx_offside_keyword = /\b(if|while|for\s+await|for|catch)\b(?!\s*[\(])/g
const rx_offside = new RegExp @ `(?:${rx_offside_op.source})|(?:${rx_offside_keyword.source})`, 'g'

const at_offside = @{}
  '::':   @{} pre: "{", post: "}", nestInner: false, implicitCommas: false,
  '::@':  @{} pre: "(", post: ")", nestInner: false, implicitCommas: false,
  '::()': @{} pre: "(", post: ")", nestInner: false, implicitCommas: false,
  '::{}': @{} pre: "{", post: "}", nestInner: false, implicitCommas: false,
  '::[]': @{} pre: "[", post: "]", nestInner: false, implicitCommas: false,

  '@':    @{} pre: "(", post: ")", nestInner: true, implicitCommas: true,
  '@:':   @{} pre: "({", post: "})", nestInner: true, implicitCommas: true
  '@#':   @{} pre: "([", post: "])", nestInner: true, implicitCommas: true,
  '@()':  @{} pre: "{", post: "}", nestInner: true, implicitCommas: true,
  '@{}':  @{} pre: "{", post: "}", nestInner: true, implicitCommas: true
  '@[]':  @{} pre: "[", post: "]", nestInner: true, implicitCommas: true,

  // note:  no '@()' -- standardize to use single-char '@ ' instead
  keyword_args: @{} pre: "(", post: ")", nestInner: false, inKeywordArg: true, implicitCommas: false,

